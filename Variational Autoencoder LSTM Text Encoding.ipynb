{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-based Variational Autoencoder for Text Encoding\n",
    "In this notebook I will demo the LSTM-based variational autoencoder (https://arxiv.org/pdf/1312.6114.pdf) I wrote in Keras for encoding text to a latent vector representation. This representation can be used for computing similarity metrics between documents (sentences, in this case) or as a feature vector for other learning tasks.\n",
    "\n",
    "The VAE is a generative model that maximizes the marginal probability of the input by conditioning it on a latent variable whose distribution is learned by a parameterized function estimator, such as a neural network. The neural network samples z from a normal distribution and transforms it to a distribution Q(z|X) to give us a distribution of z values given X which are likely to produce X. This is where the \"Variational\" part of VAE's comes in: we use KL-divergence in our loss function to drive Q(z|X) as close as we can to P(z), the prior distribution of z. The loss function also includes a reconstruction error term. In summary, the VAE learns an encoding distribution Q which produces latent representations z which are likely to produce the input data X, and a decoding function f(z) which is optimized to output data as close to X as it can from the latent representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adamm\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\adamm\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from vae_lstm import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "We will be building our dataset by converting sentences from various NLTK corpora (Brown, Reuters, Gutenberg) into a word embedding representation which will yield a 3D tensor of shape (N, S, E), where N is the number of sentences, S is the length of the sentence (zero padded at the beginning), and E is the length of the word embedding. Here we're using S=20 and E=300. We're using the wiki-news-300d-1M.vec from https://fasttext.cc/docs/en/english-vectors.html for our word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, all_text = get_data()\n",
    "\n",
    "len_train = 50000\n",
    "len_test = 10000\n",
    "train = data[:len_train]\n",
    "train_text = all_text[:len_train]\n",
    "test = data[len_train:len_train + len_test]\n",
    "\n",
    "batch_size = 50\n",
    "epochs = 30\n",
    "input_dim = train.shape[-1]\n",
    "timesteps = train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "We will train the model for 30 epochs with a batch size of 50. Our displayed loss is mean squared error between the generated word vectors in the output sequence and the word vectors in the input sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37751 samples, validate on 0 samples\n",
      "Epoch 1/30\n",
      "37751/37751 [==============================] - 64s 2ms/step - loss: 0.0071\n",
      "Epoch 2/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0069\n",
      "Epoch 3/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0068\n",
      "Epoch 4/30\n",
      "37751/37751 [==============================] - 58s 2ms/step - loss: 0.0067\n",
      "Epoch 5/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0066\n",
      "Epoch 6/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0066\n",
      "Epoch 7/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0065\n",
      "Epoch 8/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0065\n",
      "Epoch 9/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0064\n",
      "Epoch 10/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0064\n",
      "Epoch 11/30\n",
      "37751/37751 [==============================] - 58s 2ms/step - loss: 0.0063\n",
      "Epoch 12/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0063\n",
      "Epoch 13/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0063\n",
      "Epoch 14/30\n",
      "37751/37751 [==============================] - 58s 2ms/step - loss: 0.0062\n",
      "Epoch 15/30\n",
      "37751/37751 [==============================] - 58s 2ms/step - loss: 0.0062\n",
      "Epoch 16/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0062\n",
      "Epoch 17/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0062\n",
      "Epoch 18/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0061\n",
      "Epoch 19/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0061\n",
      "Epoch 20/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0061\n",
      "Epoch 21/30\n",
      "37751/37751 [==============================] - 58s 2ms/step - loss: 0.0061\n",
      "Epoch 22/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0060\n",
      "Epoch 23/30\n",
      "37751/37751 [==============================] - 58s 2ms/step - loss: 0.0060\n",
      "Epoch 24/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0060\n",
      "Epoch 25/30\n",
      "37751/37751 [==============================] - 58s 2ms/step - loss: 0.0060\n",
      "Epoch 26/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0060\n",
      "Epoch 27/30\n",
      "37751/37751 [==============================] - 60s 2ms/step - loss: 0.0059\n",
      "Epoch 28/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0059\n",
      "Epoch 29/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0059\n",
      "Epoch 30/30\n",
      "37751/37751 [==============================] - 59s 2ms/step - loss: 0.0059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24cacfa8c50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE_LSTM(input_dim=input_dim, latent_dim=100, hidden_dims=[32], timesteps=timesteps, batch_size=batch_size)\n",
    "vae, encoder, generator = model.autoencoder, model.encoder, model.generator\n",
    "\n",
    "vae.fit(train, train, shuffle=True, epochs=epochs, batch_size=batch_size, validation_data=(test, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Similar Sentences\n",
    "We'll do a spot check on our model by printing the text of the most similar sentences in the encoding space for a few training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sentences = encoder.predict(np.array(train), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[First sentence is target sentence, following are closest neighbors]\n",
      "it said after nine months following closing may require royal to register the 200 000 shares for sale . \n",
      "under terms of the letter of intent would contribute substantially to a three year exploration budget of 4 . \n",
      "to fully compensate for devaluation the quota would have to be around 28 dlrs per bag against 7 . \n",
      "economists polled by reuters said that m 1 should be anywhere from down four billion dlrs to up 2 . \n",
      "like its cousin the refrigerator a conditioner can be expected to last 20 to 25 years or more . \n",
      "economists polled by reuters said that m 1 would be anywhere from down two billion dlrs to up 1 . \n",
      "\n",
      "[First sentence is target sentence, following are closest neighbors]\n",
      "1 mln dlr defense logistics agency contract for jet fuel the defense department said . \n",
      "12 mln tonnes in 1985 the commodity board for margarine fats and oils said . \n",
      "7 mln dlrs manufactures a line of computer output to microfilm hardware and . \n",
      "13 mln tonnes editor of the hamburg based newsletter oil world said . \n",
      "87 mln ha harvested in 1986 the french maize producers ' association said . \n",
      "6 mln barrels per day bpd for the rest of 1987 oil traders said . \n",
      "\n",
      "[First sentence is target sentence, following are closest neighbors]\n",
      "he had settled accounts with him for his late agency and had talked about sundry matters of business . \n",
      "he had the black numerals on his arm so he had been branded in a concentration camp . \n",
      "he felt almost inclined to ask after all these what was a friend and what an enemy . \n",
      "he had never felt so much alive before and yet he was like a man in a trance . \n",
      "they had ruined the radar warning system with their window they had made themselves invisible above their flares . \n",
      "jane had not been gone a quarter of an hour and they had only accomplished some views of st . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_nearest_sentences(sent_idx):\n",
    "    print(\"[First sentence is target sentence, following are closest neighbors]\")\n",
    "    for s in get_nearest_sentences(sent_idx, encoded_sentences, train):\n",
    "        print(s)\n",
    "    print()\n",
    "        \n",
    "print_nearest_sentences(12352)\n",
    "print_nearest_sentences(5226)\n",
    "print_nearest_sentences(35233)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "It looks like our model is learning some notion of sentence structure. The first example has sentences which all discuss a subject with some relationship to a number at the end of the sentence. Similarly, the second example has sentences which all begin with some number of some unit and end with a '\\[subject\\] said'. The final example has sentences which all begin with a simple '\\[pronoun\\] \\[verb\\]' structure. While sentence structure is important, the final example shows that the model may not be representing topics very well in addition to structure. Perhaps using a model like the Dirichlet Variational Autoencoder (https://arxiv.org/pdf/1811.00135.pdf), which explicitly models topics in its latent representation using a dirichlet distribution, would improve the representations in that regard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
